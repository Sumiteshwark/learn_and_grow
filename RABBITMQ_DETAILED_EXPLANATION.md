**Last Updated:** November 2025  
**Author:** SUMITESHWAR KUMAR

---

# ðŸ° **RabbitMQ Complete Guide: From Beginner to Advanced**

## ðŸ“‹ **Table of Contents**
- [What is RabbitMQ?](#what-is-rabbitmq)
- [Why Use Message Queues?](#why-use-message-queues)
- [RabbitMQ Architecture](#rabbitmq-architecture)
- [Installation and Setup](#installation-and-setup)
- [Basic Concepts](#basic-concepts)
- [Core API Methods and Terms](#core-api-methods-and-terms)
- [Exchange Types](#exchange-types)
- [Message Patterns](#message-patterns)
- [Python Examples](#python-examples)
- [NodeJs Examples](#nodejs-examples)
- [Java Examples](#java-examples)
- [Advanced Features](#advanced-features)
- [Performance Tuning](#performance-tuning)
- [Monitoring and Management](#monitoring-and-management)
- [Production Deployment](#production-deployment)
- [Troubleshooting](#troubleshooting)
- [Best Practices](#best-practices)

---

## ðŸ¤” **What is RabbitMQ?**

**RabbitMQ** is a **message broker** - a software system that enables applications to communicate by sending messages through queues. It's written in **Erlang** and implements the **AMQP (Advanced Message Queuing Protocol)** standard.

### **Key Characteristics:**
- âœ… **Asynchronous Communication**: Decouples sender and receiver
- âœ… **Reliable**: Messages persist and can be acknowledged
- âœ… **Scalable**: Supports clustering and load balancing
- âœ… **Flexible**: Multiple exchange types and routing options
- âœ… **Multi-Protocol**: AMQP, MQTT, STOMP, HTTP
- âœ… **Cross-Language**: Libraries for 20+ programming languages

### **Real-World Analogy:**
Think of RabbitMQ as a **post office**:
- **Publisher** = Person sending a letter
- **Exchange** = Mail sorting facility
- **Queue** = Mailbox
- **Consumer** = Person receiving the letter

---

## ðŸŽ¯ **Why Use Message Queues?**

### **Problems Message Queues Solve:**

#### **1. Tight Coupling Between Services**
```python
# âŒ TIGHT COUPLING - Direct API calls
def process_order(order_id):
    # Step 1: Validate payment
    payment_result = payment_service.validate_payment(order_id)

    # Step 2: Update inventory
    inventory_service.update_stock(order_id)

    # Step 3: Send notification
    email_service.send_confirmation(order_id)

    # If any step fails, entire process fails
```

```python
# âœ… LOOSE COUPLING - Message Queue
def process_order(order_id):
    # Publish message to queue - asynchronous!
    rabbitmq.publish('order-processing', {'order_id': order_id, 'step': 'validate'})
```

#### **2. Handling Peak Loads**
```python
# âŒ SYNCHRONOUS PROCESSING - System overloads during peaks
@app.route('/api/upload')
def upload_file():
    # Process immediately - server overwhelmed during traffic spikes
    process_large_file(request.files['file'])
    return "File processed"
```

```python
# âœ… ASYNCHRONOUS PROCESSING - Smooth load distribution
@app.route('/api/upload')
def upload_file():
    # Add to queue - process when resources available
    rabbitmq.publish('file-processing', {'file_id': request.files['file'].filename})
    return "File queued for processing"
```

#### **3. System Reliability**
```python
# âŒ SINGLE POINT OF FAILURE
def send_notification(user_id, message):
    try:
        email_service.send(user_id, message)  # If email service is down, everything fails
    except Exception as e:
        print(f"Failed to send email: {e}")
```

```python
# âœ… FAULT TOLERANT
def send_notification(user_id, message):
    rabbitmq.publish('email-queue', {
        'user_id': user_id,
        'message': message,
        'retries': 0
    })  # Message persists even if email service is temporarily down
```

---

## ðŸ—ï¸ **RabbitMQ Architecture**

### **Core Components:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â”‚ PUBLISHER   â”‚â”€â”€â”€â–¶â”‚  EXCHANGE   â”‚â”€â”€â”€â–¶â”‚   QUEUE     â”‚
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚             â”‚
                            â”‚ CONSUMER    â”‚
                            â”‚             â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **1. Publisher (Producer)**
- Application that sends messages
- Connects to RabbitMQ server
- Publishes messages to exchanges

#### **2. Exchange**
- Receives messages from publishers
- Routes messages to appropriate queues based on rules
- Acts as a mail sorting facility

#### **3. Queue**
- Stores messages until consumed
- FIFO (First In, First Out) by default
- Can be durable (survive restarts) or transient

#### **4. Consumer**
- Application that receives and processes messages
- Connects to RabbitMQ server
- Subscribes to queues and processes messages

### **Virtual Host (vhost)**
- Logical grouping of exchanges, queues, and bindings
- Provides isolation between different applications
- Like a virtual RabbitMQ server within a server

---

## ðŸš€ **Installation and Setup**

### **Docker Installation (Recommended for Learning)**
```bash
# Pull RabbitMQ with management plugin
docker pull rabbitmq:3-management

# Run RabbitMQ
docker run -d --name rabbitmq \
  -p 5672:5672 -p 15672:15672 \
  -e RABBITMQ_DEFAULT_USER=admin \
  -e RABBITMQ_DEFAULT_PASS=admin123 \
  rabbitmq:3-management

# Access management UI at http://localhost:15672
# Username: admin, Password: admin123
```

### **Ubuntu/Debian Installation**
```bash
# Update package list
sudo apt-get update

# Install Erlang (RabbitMQ dependency)
sudo apt-get install erlang

# Add RabbitMQ repository
echo 'deb http://www.rabbitmq.com/debian/ testing main' | sudo tee /etc/apt/sources.list.d/rabbitmq.list
wget -O- https://www.rabbitmq.com/rabbitmq-release-signing-key.asc | sudo apt-key add -

# Install RabbitMQ
sudo apt-get update
sudo apt-get install rabbitmq-server

# Enable management plugin
sudo rabbitmq-plugins enable rabbitmq_management

# Start RabbitMQ
sudo systemctl start rabbitmq-server
sudo systemctl enable rabbitmq-server

# Access management UI at http://localhost:15672
```

### **macOS Installation**
```bash
# Using Homebrew
brew install rabbitmq

# Start RabbitMQ
brew services start rabbitmq

# Enable management plugin
rabbitmq-plugins enable rabbitmq_management

# Access management UI at http://localhost:15672
```

---

## ðŸ”§ **Core API Methods and Terms**

### **Essential RabbitMQ Methods**

RabbitMQ provides several core methods that developers use to interact with the message broker:

#### **1. assertExchange(exchange, type, options)**
**Purpose**: Declares an exchange if it doesn't exist
```javascript
// JavaScript example
await channel.assertExchange('logs', 'fanout', { durable: false });

// Python example
channel.exchange_declare(exchange='logs', exchange_type='fanout')
```

**Parameters:**
- `exchange`: Exchange name
- `type`: Exchange type ('direct', 'topic', 'fanout', 'headers')
- `options`: Configuration object (durable, autoDelete, etc.)

#### **2. assertQueue(queue, options)**
**Purpose**: Declares a queue if it doesn't exist
```javascript
// JavaScript example
const queueResult = await channel.assertQueue('my-queue', { durable: true });

// Python example
result = channel.queue_declare(queue='my-queue', durable=True)
```

**Parameters:**
- `queue`: Queue name (empty string for auto-generated name)
- `options`: Queue configuration (durable, exclusive, autoDelete, etc.)

#### **3. bindQueue(queue, exchange, routingKey)**
**Purpose**: Binds a queue to an exchange with a routing key
```javascript
// JavaScript example
await channel.bindQueue('my-queue', 'my-exchange', 'routing.key');

// Python example
channel.queue_bind(exchange='my-exchange', queue='my-queue', routing_key='routing.key')
```

**Parameters:**
- `queue`: Queue name
- `exchange`: Exchange name
- `routingKey`: Routing key for binding

#### **4. publish(exchange, routingKey, message, options)**
**Purpose**: Publishes a message to an exchange
```javascript
// JavaScript example
channel.publish('my-exchange', 'my.routing.key',
  Buffer.from('Hello World'),
  { persistent: true }
);

// Python example
channel.basic_publish(exchange='my-exchange',
                     routing_key='my.routing.key',
                     body='Hello World',
                     properties=pika.BasicProperties(delivery_mode=2))
```

**Parameters:**
- `exchange`: Target exchange
- `routingKey`: Message routing key
- `message`: Message content (buffer/string)
- `options`: Message properties (persistent, headers, etc.)

#### **5. consume(queue, callback, options)**
**Purpose**: Starts consuming messages from a queue
```javascript
// JavaScript example
await channel.consume('my-queue', (msg) => {
  if (msg) {
    console.log('Received:', msg.content.toString());
    channel.ack(msg);
  }
}, { noAck: false });

// Python example
def callback(ch, method, properties, body):
    print("Received:", body)
    ch.basic_ack(delivery_tag=method.delivery_tag)

channel.basic_consume(queue='my-queue', on_message_callback=callback, auto_ack=False)
```

**Parameters:**
- `queue`: Queue to consume from
- `callback`: Function to handle messages
- `options`: Consumer options (noAck, exclusive, etc.)

#### **6. ack(message)**
**Purpose**: Acknowledges successful message processing
```javascript
// JavaScript example
channel.ack(msg);

// Python example
channel.basic_ack(delivery_tag=method.delivery_tag)
```

#### **7. nack(message, multiple, requeue)**
**Purpose**: Negatively acknowledges message (rejects)
```javascript
// JavaScript example
channel.nack(msg, false, true); // Requeue message

// Python example
channel.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
```

#### **8. prefetch(count)**
**Purpose**: Sets the prefetch count for fair dispatching
```javascript
// JavaScript example
await channel.prefetch(1);

// Python example
channel.basic_qos(prefetch_count=1)
```

#### **9. purgeQueue(queue)**
**Purpose**: Removes all messages from a queue
```javascript
// JavaScript example
await channel.purgeQueue('my-queue');

// Python example
channel.queue_purge(queue='my-queue')
```

#### **10. deleteQueue(queue, options)**
**Purpose**: Deletes a queue
```javascript
// JavaScript example
await channel.deleteQueue('my-queue', { ifUnused: true });

// Python example
channel.queue_delete(queue='my-queue', if_unused=True)
```

#### **11. deleteExchange(exchange, options)**
**Purpose**: Deletes an exchange
```javascript
// JavaScript example
await channel.deleteExchange('my-exchange', { ifUnused: true });

// Python example
channel.exchange_delete(exchange='my-exchange', if_unused=True)
```

#### **12. close()**
**Purpose**: Closes the channel or connection
```javascript
// JavaScript example
await channel.close();
await connection.close();

// Python example
channel.close()
connection.close()
```

### **Common Options and Properties**

#### **Exchange Options**
```javascript
{
  durable: true,        // Exchange survives broker restart
  autoDelete: false,    // Delete when no bindings left
  internal: false,      // Internal exchange (clients can't publish)
  alternateExchange: 'alternate-exchange'  // Alternate exchange
}
```

#### **Queue Options**
```javascript
{
  durable: true,                    // Queue survives broker restart
  exclusive: false,                 // Queue only accessible by declaring connection
  autoDelete: false,                // Delete when consumer disconnects
  arguments: {                      // Additional arguments
    'x-message-ttl': 60000,        // Message TTL in milliseconds
    'x-expires': 3600000,          // Queue expiration
    'x-max-length': 1000           // Maximum queue length
  }
}
```

#### **Message Properties**
```javascript
{
  deliveryMode: 2,                  // 1=transient, 2=persistent
  priority: 0,                      // Message priority (0-255)
  contentType: 'application/json',  // Content type
  contentEncoding: 'utf-8',         // Content encoding
  headers: {                        // Custom headers
    'custom-header': 'value'
  },
  correlationId: 'correlation-123', // Correlation ID
  replyTo: 'reply-queue',           // Reply queue
  messageId: 'message-123',         // Message ID
  timestamp: Date.now(),            // Timestamp
  userId: 'user-123',               // User ID
  appId: 'my-app'                   // Application ID
}
```

### **Key Terms and Concepts**

#### **1. Channel**
- **Definition**: Lightweight connection within a connection
- **Purpose**: Multiplexes a single TCP connection
- **Usage**: Most operations happen on channels, not connections
- **Best Practice**: One channel per thread/fiber

#### **2. Connection**
- **Definition**: TCP connection to RabbitMQ server
- **Purpose**: Establishes communication with broker
- **Usage**: Created once, then channels are created from it
- **Best Practice**: Long-lived, reused across operations

#### **3. Durable**
- **Definition**: Survives broker restarts
- **Usage**: `durable: true` for exchanges and queues
- **Trade-off**: Performance vs reliability

#### **4. Transient**
- **Definition**: Lost when broker restarts
- **Usage**: `durable: false` for temporary exchanges/queues
- **Trade-off**: Performance vs reliability

#### **5. Exclusive**
- **Definition**: Only accessible by declaring connection
- **Usage**: Temporary queues, private communications
- **Auto-delete**: Removed when connection closes

#### **6. Auto-delete**
- **Definition**: Automatically deleted when no longer used
- **Usage**: Temporary exchanges/queues
- **Triggers**: Last consumer disconnects, last binding removed

#### **7. Routing Key**
- **Definition**: Message attribute used for routing
- **Usage**: Determines which queues receive the message
- **Exchange Types**: Used differently by each exchange type

#### **8. Binding**
- **Definition**: Link between exchange and queue
- **Purpose**: Defines message routing rules
- **Components**: Exchange name, queue name, routing key/pattern

#### **9. Consumer Tag**
- **Definition**: Unique identifier for a consumer
- **Usage**: Cancel specific consumers
- **Generated**: Auto-generated or user-provided

#### **10. Delivery Tag**
- **Definition**: Unique identifier for a message delivery
- **Usage**: Acknowledge or reject specific messages
- **Scope**: Unique per channel

### **Error Handling Methods**

#### **1. onError(callback)**
**Purpose**: Handle connection/channel errors
```javascript
connection.on('error', (err) => {
  console.error('Connection error:', err);
});

channel.on('error', (err) => {
  console.error('Channel error:', err);
});
```

#### **2. onClose(callback)**
**Purpose**: Handle connection/channel closure
```javascript
connection.on('close', () => {
  console.log('Connection closed');
});

channel.on('close', () => {
  console.log('Channel closed');
});
```

### **Advanced Methods**

#### **1. confirmChannel()**
**Purpose**: Create a channel with publisher confirms
```javascript
const confirmChannel = await connection.createConfirmChannel();
```

#### **2. checkExchange(exchange)**
**Purpose**: Verify exchange exists without creating it
```javascript
await channel.checkExchange('my-exchange');
```

#### **3. checkQueue(queue)**
**Purpose**: Verify queue exists without creating it
```javascript
const result = await channel.checkQueue('my-queue');
```

#### **4. get(queue, options)**
**Purpose**: Get a single message from queue (polling)
```javascript
const msg = await channel.get('my-queue', { noAck: false });
```

#### **5. recover()**
**Purpose**: Redeliver unacknowledged messages
```javascript
await channel.recover();
```

### **Common Patterns**

#### **1. Publisher Pattern**
```javascript
async function publishMessage(exchange, routingKey, message) {
  try {
    await channel.assertExchange(exchange, 'direct', { durable: true });
    const published = channel.publish(exchange, routingKey,
      Buffer.from(JSON.stringify(message)),
      { persistent: true }
    );
    return published;
  } catch (error) {
    console.error('Publish error:', error);
    throw error;
  }
}
```

#### **2. Consumer Pattern**
```javascript
async function startConsumer(queue, exchange, routingKey) {
  try {
    await channel.assertQueue(queue, { durable: true });
    await channel.assertExchange(exchange, 'direct', { durable: true });
    await channel.bindQueue(queue, exchange, routingKey);

    await channel.consume(queue, async (msg) => {
      if (msg) {
        try {
          const message = JSON.parse(msg.content.toString());
          await processMessage(message);
          channel.ack(msg);
        } catch (error) {
          console.error('Processing error:', error);
          channel.nack(msg, false, true); // Requeue
        }
      }
    }, { noAck: false });

    console.log(`Consumer started for queue: ${queue}`);
  } catch (error) {
    console.error('Consumer setup error:', error);
    throw error;
  }
}
```

#### **3. Connection Management Pattern**
```javascript
class RabbitMQConnection {
  constructor() {
    this.connection = null;
    this.channel = null;
  }

  async connect() {
    if (!this.connection) {
      this.connection = await amqp.connect(process.env.RABBITMQ_URL);
      this.channel = await this.connection.createChannel();
    }
    return this.channel;
  }

  async close() {
    if (this.channel) await this.channel.close();
    if (this.connection) await this.connection.close();
  }
}
```

---

## ðŸ“š **Basic Concepts**

### **Hello World Example - Python**

First, let's understand the basic workflow with a simple example:

```python
# producer.py
import pika

# Connect to RabbitMQ
connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
channel = connection.channel()

# Declare a queue
channel.queue_declare(queue='hello')

# Send a message
channel.basic_publish(exchange='', routing_key='hello', body='Hello World!')

print(" [x] Sent 'Hello World!'")

# Close connection
connection.close()
```

```python
# consumer.py
import pika

def callback(ch, method, properties, body):
    print(f" [x] Received {body}")

# Connect to RabbitMQ
connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
channel = connection.channel()

# Declare the same queue
channel.queue_declare(queue='hello')

# Set up consumer with callback
channel.basic_consume(queue='hello', on_message_callback=callback, auto_ack=True)

print(' [*] Waiting for messages. To exit press CTRL+C')

# Start consuming
channel.start_consuming()
```

### **Key Points from Hello World:**

1. **Connection**: Both producer and consumer connect to RabbitMQ server
2. **Queue Declaration**: Both declare the same queue (idempotent operation)
3. **Publishing**: Producer sends message to exchange with routing key
4. **Consuming**: Consumer subscribes to queue and receives messages
5. **Auto ACK**: Messages are automatically acknowledged when received

---

## ðŸ”„ **Exchange Types**

RabbitMQ has **4 main exchange types**, each with different routing logic:

### **1. Direct Exchange**
Routes messages to queues whose binding key **exactly matches** the routing key.

```python
# Declare direct exchange
channel.exchange_declare(exchange='direct_logs', exchange_type='direct')

# Producer sends with routing key
channel.basic_publish(
    exchange='direct_logs',
    routing_key='error',  # or 'info', 'warning', 'debug'
    body='This is an error message'
)

# Consumer binds with specific key
channel.queue_bind(exchange='direct_logs', queue='error_queue', routing_key='error')
```

**Use Case**: Routing messages to specific queues based on exact criteria (like log levels).

### **2. Topic Exchange**
Routes messages to queues whose binding key **matches a pattern** using wildcards.

```python
# Declare topic exchange
channel.exchange_declare(exchange='topic_logs', exchange_type='topic')

# Wildcard patterns:
# * = matches exactly one word
# # = matches zero or more words

# Producer sends with routing key
channel.basic_publish(
    exchange='topic_logs',
    routing_key='user.login.success',
    body='User logged in successfully'
)

# Consumer binds with pattern
channel.queue_bind(exchange='topic_logs', queue='user_logs', routing_key='user.*')
channel.queue_bind(exchange='topic_logs', queue='login_logs', routing_key='*.login.*')
channel.queue_bind(exchange='topic_logs', queue='all_logs', routing_key='#')
```

**Use Case**: Flexible routing based on hierarchical topics (like `user.auth.login`).

### **3. Fanout Exchange**
Routes messages to **all bound queues** regardless of routing key.

```python
# Declare fanout exchange
channel.exchange_declare(exchange='logs', exchange_type='fanout')

# All bound queues receive ALL messages
channel.basic_publish(exchange='logs', routing_key='', body='Broadcast message')

# Multiple consumers can bind to same exchange
channel.queue_bind(exchange='logs', queue='queue1')
channel.queue_bind(exchange='logs', queue='queue2')
channel.queue_bind(exchange='logs', queue='queue3')
```

**Use Case**: Broadcasting messages to multiple consumers (like log aggregation).

### **4. Headers Exchange**
Routes messages based on **message header attributes** instead of routing key.

```python
# Declare headers exchange
channel.exchange_declare(exchange='headers_exchange', exchange_type='headers')

# Producer sends with headers
channel.basic_publish(
    exchange='headers_exchange',
    routing_key='',  # Not used with headers exchange
    body='Message with headers',
    properties=pika.BasicProperties(
        headers={
            'format': 'pdf',
            'type': 'report',
            'urgent': True
        }
    )
)

# Consumer binds with header matching
channel.queue_bind(
    exchange='headers_exchange',
    queue='pdf_reports',
    arguments={'format': 'pdf', 'type': 'report', 'x-match': 'all'}  # All headers must match
)
```

**Use Case**: Complex routing based on multiple attributes.

---

## ðŸ“¨ **Message Patterns**

### **1. Work Queues (Task Distribution)**
Distribute tasks among multiple workers.

```python
# Producer (Task Publisher)
def send_task(task):
    channel.basic_publish(
        exchange='',
        routing_key='task_queue',
        body=task,
        properties=pika.BasicProperties(
            delivery_mode=2,  # Make message persistent
        )
    )

# Consumer (Worker)
def callback(ch, method, properties, body):
    print(f"Processing task: {body}")
    # Simulate work
    time.sleep(body.count(b'.'))
    print("Task completed")
    ch.basic_ack(delivery_tag=method.delivery_tag)  # Acknowledge message
```

### **2. Publish/Subscribe**
One-to-many communication pattern.

```python
# Publisher
channel.exchange_declare(exchange='broadcast', exchange_type='fanout')
channel.basic_publish(exchange='broadcast', routing_key='', body='Hello everyone!')

# Subscribers
channel.exchange_declare(exchange='broadcast', exchange_type='fanout')
result = channel.queue_declare(queue='', exclusive=True)  # Auto-generated queue name
channel.queue_bind(exchange='broadcast', queue=result.method.queue)
channel.basic_consume(queue=result.method.queue, on_message_callback=callback)
```

### **3. RPC (Remote Procedure Call)**
Request-response pattern over messaging.

```python
# Client (RPC Caller)
def call_server(n):
    # Generate correlation ID
    corr_id = str(uuid.uuid4())

    # Send request
    channel.basic_publish(
        exchange='',
        routing_key='rpc_queue',
        properties=pika.BasicProperties(
            reply_to=callback_queue,
            correlation_id=corr_id,
        ),
        body=str(n)
    )

    # Wait for response
    while response is None:
        connection.process_data_events()
    return response

# Server (RPC Responder)
def on_request(ch, method, props, body):
    n = int(body)
    response = fibonacci(n)

    ch.basic_publish(
        exchange='',
        routing_key=props.reply_to,
        properties=pika.BasicProperties(correlation_id=props.correlation_id),
        body=str(response)
    )
    ch.basic_ack(delivery_tag=method.delivery_tag)
```

---

## ðŸ **Python Examples**

### **Complete Producer-Consumer Example**

```python
#!/usr/bin/env python3
import pika
import json
import time
from datetime import datetime

class RabbitMQClient:
    def __init__(self, host='localhost', port=5672, username='guest', password='guest'):
        self.credentials = pika.PlainCredentials(username, password)
        self.parameters = pika.ConnectionParameters(
            host=host,
            port=port,
            credentials=self.credentials,
            heartbeat=600,
            blocked_connection_timeout=300
        )

    def create_connection(self):
        return pika.BlockingConnection(self.parameters)

# Producer
class Producer(RabbitMQClient):
    def send_message(self, exchange, routing_key, message, exchange_type='direct'):
        connection = self.create_connection()
        channel = connection.channel()

        # Declare exchange
        channel.exchange_declare(exchange=exchange, exchange_type=exchange_type, durable=True)

        # Send message
        channel.basic_publish(
            exchange=exchange,
            routing_key=routing_key,
            body=json.dumps(message),
            properties=pika.BasicProperties(
                delivery_mode=2,  # Persistent message
                timestamp=int(time.time())
            )
        )

        print(f"Sent message to {exchange}:{routing_key}")
        connection.close()

# Consumer
class Consumer(RabbitMQClient):
    def consume_messages(self, exchange, queue_name, routing_key='', exchange_type='direct'):
        connection = self.create_connection()
        channel = connection.channel()

        # Declare exchange and queue
        channel.exchange_declare(exchange=exchange, exchange_type=exchange_type, durable=True)
        channel.queue_declare(queue=queue_name, durable=True)

        # Bind queue to exchange
        channel.queue_bind(exchange=exchange, queue=queue_name, routing_key=routing_key)

        # Set up consumer
        def callback(ch, method, properties, body):
            message = json.loads(body)
            print(f"Received: {message}")

            # Simulate processing time
            time.sleep(1)

            # Acknowledge message
            ch.basic_ack(delivery_tag=method.delivery_tag)

        channel.basic_consume(queue=queue_name, on_message_callback=callback)
        print(f"Waiting for messages in {queue_name}. Press CTRL+C to exit")
        channel.start_consuming()

# Usage
if __name__ == "__main__":
    # Producer example
    producer = Producer()
    message = {
        'id': '12345',
        'action': 'process_data',
        'data': {'key': 'value'},
        'timestamp': datetime.now().isoformat()
    }
    producer.send_message('data_processing', 'process', message)

    # Consumer example (run in separate terminal)
    # consumer = Consumer()
    # consumer.consume_messages('data_processing', 'data_queue', 'process')
```

---

## ðŸŸ¨ **NodeJs Examples**

### **Express.js with RabbitMQ**

```javascript
const express = require('express');
const amqp = require('amqplib');

const app = express();
app.use(express.json());

class RabbitMQService {
    constructor() {
        this.connection = null;
        this.channel = null;
    }

    async connect() {
        try {
            this.connection = await amqp.connect('amqp://localhost');
            this.channel = await this.connection.createChannel();
            console.log('Connected to RabbitMQ');
        } catch (error) {
            console.error('Failed to connect to RabbitMQ:', error);
        }
    }

    async publishMessage(exchange, routingKey, message, exchangeType = 'direct') {
        if (!this.channel) await this.connect();

        // Declare exchange
        await this.channel.assertExchange(exchange, exchangeType, { durable: true });

        // Publish message
        this.channel.publish(
            exchange,
            routingKey,
            Buffer.from(JSON.stringify(message)),
            { persistent: true }
        );

        console.log(`Message sent to ${exchange}:${routingKey}`);
    }

    async consumeMessages(queueName, callback) {
        if (!this.channel) await this.connect();

        // Declare queue
        const queue = await this.channel.assertQueue(queueName, { durable: true });

        // Set up consumer
        this.channel.consume(queueName, async (msg) => {
            if (msg) {
                try {
                    const message = JSON.parse(msg.content.toString());
                    await callback(message);

                    // Acknowledge message
                    this.channel.ack(msg);
                } catch (error) {
                    console.error('Error processing message:', error);
                    // Reject message and don't requeue
                    this.channel.nack(msg, false, false);
                }
            }
        }, { noAck: false });

        console.log(`Consuming messages from ${queueName}`);
    }
}

// Usage
const rabbitMQ = new RabbitMQService();

// API endpoint that publishes messages
app.post('/api/process', async (req, res) => {
    try {
        await rabbitMQ.connect();

        const message = {
            id: Date.now().toString(),
            action: 'process_data',
            data: req.body,
            timestamp: new Date().toISOString()
        };

        await rabbitMQ.publishMessage('data_processing', 'process', message);

        res.json({ status: 'Message queued for processing' });
    } catch (error) {
        res.status(500).json({ error: error.message });
    }
});

// Start consumer in background
async function startConsumer() {
    await rabbitMQ.connect();

    await rabbitMQ.consumeMessages('data_queue', async (message) => {
        console.log('Processing message:', message);

        // Simulate processing
        await new Promise(resolve => setTimeout(resolve, 2000));

        console.log('Message processed successfully');
    });
}

startConsumer();

app.listen(3000, () => {
    console.log('API server running on port 3000');
});
```

---

## â˜• **Java Examples**

### **Spring Boot with RabbitMQ**

```xml
<!-- pom.xml -->
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-amqp</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
</dependencies>
```

```java
// RabbitMQ Configuration
@Configuration
public class RabbitMQConfig {

    public static final String EXCHANGE_NAME = "message_exchange";
    public static final String QUEUE_NAME = "message_queue";
    public static final String ROUTING_KEY = "message_routing_key";

    @Bean
    public Queue queue() {
        return new Queue(QUEUE_NAME, true); // Durable queue
    }

    @Bean
    public DirectExchange exchange() {
        return new DirectExchange(EXCHANGE_NAME, true, false); // Durable, not auto-delete
    }

    @Bean
    public Binding binding(Queue queue, DirectExchange exchange) {
        return BindingBuilder.bind(queue).to(exchange).with(ROUTING_KEY);
    }

    @Bean
    public MessageConverter jsonMessageConverter() {
        return new Jackson2JsonMessageConverter();
    }

    @Bean
    public AmqpTemplate amqpTemplate(ConnectionFactory connectionFactory) {
        RabbitTemplate rabbitTemplate = new RabbitTemplate(connectionFactory);
        rabbitTemplate.setMessageConverter(jsonMessageConverter());
        return rabbitTemplate;
    }
}

// Message Model
public class MessageData {
    private String id;
    private String content;
    private LocalDateTime timestamp;

    // Getters and setters
}

// Producer Service
@Service
public class MessageProducer {

    @Autowired
    private AmqpTemplate amqpTemplate;

    public void sendMessage(MessageData message) {
        try {
            amqpTemplate.convertAndSend(
                RabbitMQConfig.EXCHANGE_NAME,
                RabbitMQConfig.ROUTING_KEY,
                message
            );
            System.out.println("Message sent: " + message.getId());
        } catch (Exception e) {
            System.err.println("Failed to send message: " + e.getMessage());
        }
    }
}

// Consumer Service
@Service
public class MessageConsumer {

    @RabbitListener(queues = RabbitMQConfig.QUEUE_NAME)
    public void receiveMessage(MessageData message, Message amqpMessage) throws Exception {
        System.out.println("Received message: " + message.getId());

        try {
            // Process the message
            processMessage(message);

            // Acknowledge the message
            amqpMessage.getMessageProperties().setHeader("processed", true);

        } catch (Exception e) {
            System.err.println("Error processing message: " + e.getMessage());

            // Reject the message and don't requeue
            throw new AmqpRejectAndDontRequeueException("Processing failed");
        }
    }

    private void processMessage(MessageData message) throws InterruptedException {
        // Simulate processing time
        Thread.sleep(2000);
        System.out.println("Message processed: " + message.getContent());
    }
}

// REST Controller
@RestController
@RequestMapping("/api/messages")
public class MessageController {

    @Autowired
    private MessageProducer producer;

    @PostMapping
    public ResponseEntity<String> sendMessage(@RequestBody MessageData message) {
        message.setId(UUID.randomUUID().toString());
        message.setTimestamp(LocalDateTime.now());

        producer.sendMessage(message);

        return ResponseEntity.ok("Message sent successfully");
    }
}

// Application Properties
# application.properties
spring.rabbitmq.host=localhost
spring.rabbitmq.port=5672
spring.rabbitmq.username=guest
spring.rabbitmq.password=guest
spring.rabbitmq.virtual-host=/

# Connection settings
spring.rabbitmq.connection-timeout=60000
spring.rabbitmq.requested-heartbeat=60

# Listener settings
spring.rabbitmq.listener.simple.concurrency=5
spring.rabbitmq.listener.simple.max-concurrency=10
spring.rabbitmq.listener.simple.prefetch=1
```

---

## ðŸ“‹ **Predefined Variables and Configuration**

### **Environment Variables**

RabbitMQ uses numerous environment variables for configuration and runtime behavior:

#### **Core Authentication Variables**
```bash
# Default user credentials
RABBITMQ_DEFAULT_USER=admin                    # Default username
RABBITMQ_DEFAULT_PASS=changeme123              # Default password
RABBITMQ_DEFAULT_VHOST=/                       # Default virtual host

# Guest user configuration
RABBITMQ_ALLOW_ANONYMOUS_LOGIN=false           # Disable anonymous login
RABBITMQ_GUEST_USER=guest                      # Guest username
RABBITMQ_GUEST_PASS=guest                      # Guest password
```

#### **Network Configuration Variables**
```bash
# Server binding
RABBITMQ_NODE_IP_ADDRESS=0.0.0.0               # Bind to all interfaces
RABBITMQ_NODE_PORT=5672                        # AMQP port
RABBITMQ_MANAGEMENT_PORT=15672                 # Management UI port
```

#### **Clustering Variables**
```bash
# Cluster configuration
RABBITMQ_CLUSTER_NODES="rabbit@node1,rabbit@node2"  # Cluster nodes
RABBITMQ_CLUSTER_PARTITION_HANDLING=pause_minority  # Partition handling
RABBITMQ_NODE_NAME=rabbit@hostname               # Node name
RABBITMQ_USE_LONGNAME=false                      # Use short/long names
```

### **Runtime Variables and Constants**

#### **Message Properties**
```python
# Message priorities
MIN_PRIORITY = 0                               # Minimum priority
MAX_PRIORITY = 255                             # Maximum priority

# Exchange types
EXCHANGE_TYPE_DIRECT = 'direct'                # Direct exchange
EXCHANGE_TYPE_TOPIC = 'topic'                  # Topic exchange
EXCHANGE_TYPE_FANOUT = 'fanout'                # Fanout exchange
EXCHANGE_TYPE_HEADERS = 'headers'              # Headers exchange
```

### **System Variables and Paths**

#### **Default File Paths**
```bash
# Linux/Unix paths
RABBITMQ_BASE=/var/lib/rabbitmq                 # Base directory
RABBITMQ_CONFIG_FILE=/etc/rabbitmq/rabbitmq.conf  # Configuration file
RABBITMQ_LOG_BASE=/var/log/rabbitmq            # Log directory
RABBITMQ_PID_FILE=/var/run/rabbitmq.pid        # PID file
RABBITMQ_PLUGINS_DIR=/usr/lib/rabbitmq/plugins  # Plugins directory

# macOS paths (Homebrew)
RABBITMQ_BASE=/usr/local/var/lib/rabbitmq      # Base directory
RABBITMQ_CONFIG_FILE=/usr/local/etc/rabbitmq/rabbitmq.conf  # Config file
RABBITMQ_LOG_BASE=/usr/local/var/log/rabbitmq  # Log directory
```

### **Docker Variables**

#### **Docker Environment Variables**
```dockerfile
# Docker-specific variables
RABBITMQ_NODE_IP_ADDRESS=0.0.0.0               # Bind to container interfaces
RABBITMQ_DEFAULT_USER=${RABBITMQ_USER}         # User from environment
RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}     # Password from environment

# Docker health checks
RABBITMQ_HEALTHCHECK_TIMEOUT=60                # Health check timeout
RABBITMQ_HEALTHCHECK_FAILURE_THRESHOLD=3       # Failure threshold
```

#### **Management API Variables**
```python
# Management API endpoints
API_OVERVIEW = '/api/overview'                  # Broker overview
API_QUEUES = '/api/queues'                     # Queue information
API_EXCHANGES = '/api/exchanges'               # Exchange information
API_CONNECTIONS = '/api/connections'           # Connection information
API_NODES = '/api/nodes'                       # Node information
```

---

## ðŸ”§ **Advanced Features**

### **Message Acknowledgments**
Control when messages are considered successfully processed:

```python
# Manual acknowledgment
def callback(ch, method, properties, body):
    try:
        # Process message
        process_message(body)

        # Explicitly acknowledge
        ch.basic_ack(delivery_tag=method.delivery_tag)
    except Exception as e:
        print(f"Error processing message: {e}")

        # Reject message and requeue
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)

# Set prefetch count (how many unacked messages per consumer)
channel.basic_qos(prefetch_count=1)
```

### **Message TTL (Time To Live)**
Automatically expire messages:

```python
# Message-level TTL
channel.basic_publish(
    exchange='exchange',
    routing_key='key',
    body='message',
    properties=pika.BasicProperties(
        expiration='60000'  # Expire after 60 seconds
    )
)

# Queue-level TTL
channel.queue_declare(
    queue='ttl_queue',
    arguments={'x-message-ttl': 60000}  # 60 seconds
)
```

### **Dead Letter Exchange**
Handle messages that can't be processed:

```python
# Declare dead letter exchange and queue
channel.exchange_declare(exchange='dlx', exchange_type='direct')
channel.queue_declare(queue='dlq')
channel.queue_bind(exchange='dlx', queue='dlq', routing_key='failed')

# Main queue with dead letter configuration
channel.queue_declare(
    queue='main_queue',
    arguments={
        'x-dead-letter-exchange': 'dlx',
        'x-dead-letter-routing-key': 'failed',
        'x-message-ttl': 30000  # 30 seconds TTL
    }
)
```

### **Message Priorities**
Process important messages first:

```python
# Declare queue with priority support
channel.queue_declare(
    queue='priority_queue',
    arguments={'x-max-priority': 10}  # Priority levels 0-10
)

# Send message with priority
channel.basic_publish(
    exchange='',
    routing_key='priority_queue',
    body='Important message',
    properties=pika.BasicProperties(priority=10)
)
```

### **Publisher Confirms**
Ensure messages are safely received by RabbitMQ:

```python
# Enable publisher confirms
channel.confirm_delivery()

def publish_with_confirm(channel, exchange, routing_key, message):
    try:
        channel.basic_publish(
            exchange=exchange,
            routing_key=routing_key,
            body=message,
            properties=pika.BasicProperties(delivery_mode=2),
            mandatory=True
        )
        print("Message published successfully")
    except pika.exceptions.UnroutableError:
        print("Message could not be routed")
    except Exception as e:
        print(f"Failed to publish: {e}")
```

---

## âš¡ **Performance Tuning**

### **Connection Optimization**
```python
# Connection pooling
connection_params = pika.ConnectionParameters(
    host='localhost',
    port=5672,
    credentials=credentials,
    heartbeat=600,
    blocked_connection_timeout=300,
    connection_attempts=3,
    retry_delay=5
)

# Use connection pool for multiple operations
from pika import BlockingConnection

class ConnectionPool:
    def __init__(self, max_connections=10):
        self.pool = []
        self.max_connections = max_connections

    def get_connection(self):
        if len(self.pool) < self.max_connections:
            conn = BlockingConnection(connection_params)
            self.connections.append(conn)
            return conn
        else:
            return self.pool[0]  # Reuse existing connection
```

### **Channel Optimization**
```python
# Multiple channels per connection
connection = pika.BlockingConnection(connection_params)

# Create separate channels for different operations
publish_channel = connection.channel()
consume_channel = connection.channel()

# Set different prefetch counts
publish_channel.basic_qos(prefetch_count=1)
consume_channel.basic_qos(prefetch_count=10)
```

### **Batch Operations**
```python
# Batch publishing for better throughput
def batch_publish(channel, messages, batch_size=100):
    batch = []

    for message in messages:
        batch.append(message)

        if len(batch) >= batch_size:
            # Publish entire batch
            for msg in batch:
                channel.basic_publish(
                    exchange=msg['exchange'],
                    routing_key=msg['routing_key'],
                    body=msg['body']
                )
            batch = []

    # Publish remaining messages
    for msg in batch:
        channel.basic_publish(
            exchange=msg['exchange'],
            routing_key=msg['routing_key'],
            body=msg['body']
        )
```

### **Memory Optimization**
```python
# Monitor memory usage
def get_memory_usage():
    import psutil
    import os

    process = psutil.Process(os.getpid())
    return {
        'rss': process.memory_info().rss / 1024 / 1024,  # MB
        'vms': process.memory_info().vms / 1024 / 1024,  # MB
        'percent': process.memory_percent()
    }

# Optimize queue declaration
channel.queue_declare(
    queue='optimized_queue',
    durable=True,
    arguments={
        'x-queue-mode': 'lazy',  # Keep messages in disk, not memory
        'x-max-length': 10000,  # Limit queue length
        'x-overflow': 'reject-publish'  # Reject when queue is full
    }
)
```

---

## ðŸ“Š **Monitoring and Management**

### **Management API**
```python
import requests

class RabbitMQMonitor:
    def __init__(self, host='localhost', port=15672, username='guest', password='guest'):
        self.base_url = f'http://{host}:{port}/api'
        self.auth = (username, password)

    def get_overview(self):
        """Get broker overview"""
        response = requests.get(f'{self.base_url}/overview', auth=self.auth)
        return response.json()

    def get_queues(self):
        """Get all queues"""
        response = requests.get(f'{self.base_url}/queues', auth=self.auth)
        return response.json()

    def get_queue_info(self, vhost, queue_name):
        """Get specific queue information"""
        response = requests.get(f'{self.base_url}/queues/{vhost}/{queue_name}', auth=self.auth)
        return response.json()

    def get_exchanges(self):
        """Get all exchanges"""
        response = requests.get(f'{self.base_url}/exchanges', auth=self.auth)
        return response.json()

    def get_connections(self):
        """Get active connections"""
        response = requests.get(f'{self.base_url}/connections', auth=self.auth)
        return response.json()

# Usage
monitor = RabbitMQMonitor()
overview = monitor.get_overview()
print(f"Total queues: {overview['object_totals']['queues']}")
print(f"Total connections: {overview['object_totals']['connections']}")
```

### **Health Checks**
```python
def check_rabbitmq_health(host='localhost', port=5672):
    """Check if RabbitMQ is healthy"""
    try:
        connection = pika.BlockingConnection(
            pika.ConnectionParameters(host=host, port=port)
        )
        connection.close()
        return True
    except Exception as e:
        print(f"RabbitMQ health check failed: {e}")
        return False

def check_queue_health(queue_name, min_length=0, max_length=1000):
    """Check if queue is within healthy bounds"""
    try:
        connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
        channel = connection.channel()

        # Get queue info
        queue_info = channel.queue_declare(queue=queue_name, passive=True)

        message_count = queue_info.method.message_count
        consumer_count = queue_info.method.consumer_count

        connection.close()

        # Check bounds
        if message_count < min_length:
            print(f"Queue {queue_name} has too few messages: {message_count}")
            return False

        if message_count > max_length:
            print(f"Queue {queue_name} has too many messages: {message_count}")
            return False

        if consumer_count == 0:
            print(f"Queue {queue_name} has no consumers")
            return False

        return True

    except Exception as e:
        print(f"Queue health check failed: {e}")
        return False
```

---

## ðŸš€ **Production Deployment**

### **Docker Compose for Production**
```yaml
version: '3.8'

services:
  rabbitmq:
    image: rabbitmq:3.12-management-alpine
    container_name: rabbitmq_prod
    hostname: rabbitmq-prod
    ports:
      - "5672:5672"    # AMQP port
      - "15672:15672"  # Management UI
      - "15692:15692"  # Prometheus metrics
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD}
      RABBITMQ_DEFAULT_VHOST: /
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
      - ./rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
      - ./enabled_plugins:/etc/rabbitmq/enabled_plugins:ro
    networks:
      - rabbitmq_network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Load balancer (optional)
  haproxy:
    image: haproxy:2.7-alpine
    ports:
      - "5670:5670"  # Load balanced AMQP
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
    depends_on:
      - rabbitmq
    networks:
      - rabbitmq_network

volumes:
  rabbitmq_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /data/rabbitmq

networks:
  rabbitmq_network:
    driver: bridge
```

### **RabbitMQ Configuration File**
```ini
# rabbitmq.conf
# Core settings
listeners.tcp.default = 5672
management.tcp.port = 15672

# Security
default_user = admin
default_pass = changeme123
loopback_users = none

# Performance tuning
tcp_listen_options.backlog = 128
tcp_listen_options.nodelay = true
tcp_listen_options.sndbuf = 196608
tcp_listen_options.recbuf = 196608

# Memory
vm_memory_high_watermark.absolute = 2GB
vm_memory_high_watermark_paging_ratio = 0.75

# Disk free limit
disk_free_limit.absolute = 2GB

# Clustering (if using cluster)
cluster_formation.peer_discovery_backend = classic_config
cluster_formation.classic_config.nodes.1 = rabbit@rabbitmq-1
cluster_formation.classic_config.nodes.2 = rabbit@rabbitmq-2
cluster_formation.classic_config.nodes.3 = rabbit@rabbitmq-3

# Federation (if using federation)
federation-upstream-set = upstream
federation-upstream.upstream.uri = amqp://upstream-server
federation-upstream.upstream.expires = 3600000

# Shovel (if using shovel)
shovel.name = replicate-shovel
shovel.type = dynamic
shovel.definition.type = one_to_one
shovel.definition.src-uri = amqp://source
shovel.definition.dest-uri = amqp://destination
```

### **Kubernetes Deployment**
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rabbitmq-cluster
  labels:
    app: rabbitmq
spec:
  serviceName: rabbitmq
  replicas: 3
  selector:
    matchLabels:
      app: rabbitmq
  template:
    metadata:
      labels:
        app: rabbitmq
    spec:
      containers:
      - name: rabbitmq
        image: rabbitmq:3.12-management-alpine
        ports:
        - containerPort: 5672
          name: amqp
        - containerPort: 15672
          name: management
        - containerPort: 4369
          name: epmd
        - containerPort: 25672
          name: clustering
        env:
        - name: RABBITMQ_ERLANG_COOKIE
          value: "SWQOKODSQALRPCLNMEQG"
        - name: RABBITMQ_DEFAULT_USER
          valueFrom:
            secretKeyRef:
              name: rabbitmq-secret
              key: username
        - name: RABBITMQ_DEFAULT_PASS
          valueFrom:
            secretKeyRef:
              name: rabbitmq-secret
              key: password
        volumeMounts:
        - name: rabbitmq-data
          mountPath: /var/lib/rabbitmq
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          exec:
            command: ["rabbitmq-diagnostics", "ping"]
          initialDelaySeconds: 60
          periodSeconds: 60
          timeoutSeconds: 15
        readinessProbe:
          exec:
            command: ["rabbitmq-diagnostics", "ping"]
          initialDelaySeconds: 20
          periodSeconds: 10
          timeoutSeconds: 10
      volumes:
      - name: rabbitmq-data
        persistentVolumeClaim:
          claimName: rabbitmq-pvc
  volumeClaimTemplates:
  - metadata:
      name: rabbitmq-pvc
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 50Gi
```

---

## ðŸ”§ **Troubleshooting**

### **Common Issues and Solutions**

#### **1. Connection Refused**
```bash
# Check if RabbitMQ is running
sudo systemctl status rabbitmq-server

# Check port availability
netstat -tlnp | grep 5672

# Check firewall
sudo ufw status
sudo ufw allow 5672

# Check Docker container
docker ps | grep rabbitmq
docker logs <container_id>
```

#### **2. High Memory Usage**
```bash
# Check memory usage
rabbitmqctl status | grep memory

# Check queue memory usage
rabbitmqctl list_queues name memory

# Reduce memory watermark
echo "vm_memory_high_watermark.relative = 0.8" >> /etc/rabbitmq/rabbitmq.conf

# Enable lazy queues
rabbitmqctl eval 'application:set_env(rabbit, lazy_queue_explicit_gc_run_operation_threshold, 1000).'
```

#### **3. Slow Performance**
```bash
# Check connection count
rabbitmqctl list_connections

# Check channel count
rabbitmqctl list_channels

# Check queue length
rabbitmqctl list_queues name messages

# Enable flow control
echo "vm_memory_high_watermark.relative = 0.6" >> /etc/rabbitmq/rabbitmq.conf

# Optimize Erlang VM
echo "RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=\"+P 1048576 +t 5000000\"" >> /etc/rabbitmq/rabbitmq-env.conf
```

#### **4. Messages Not Being Consumed**
```bash
# Check consumer count
rabbitmqctl list_queues name consumers

# Check consumer connections
rabbitmqctl list_consumers

# Check consumer prefetch
rabbitmqctl list_channels consumer_count prefetch_count

# Check for dead letters
rabbitmqctl list_queues name messages_dead_letter
```

#### **5. Cluster Issues**
```bash
# Check cluster status
rabbitmqctl cluster_status

# Check node health
rabbitmqctl node_health_check

# Reset node
rabbitmqctl stop_app
rabbitmqctl reset
rabbitmqctl start_app

# Force reset (dangerous)
rabbitmqctl force_reset
```

### **Debug Commands**
```bash
# View broker logs
sudo tail -f /var/log/rabbitmq/rabbit@hostname.log

# Check Erlang processes
ps aux | grep beam

# Monitor network connections
netstat -tlnp | grep :5672

# Check disk usage
du -h /var/lib/rabbitmq

# View management UI logs
curl -u guest:guest http://localhost:15672/api/nodes
```

### **Performance Tuning**
```bash
# Optimize connection settings
echo "tcp_listen_options.backlog = 128" >> /etc/rabbitmq/rabbitmq.conf
echo "tcp_listen_options.nodelay = true" >> /etc/rabbitmq/rabbitmq.conf

# Channel optimization
echo "channel_max = 2047" >> /etc/rabbitmq/rabbitmq.conf

# Queue optimization
echo "queue_index_embed_msgs_below = 4096" >> /etc/rabbitmq/rabbitmq.conf

# Enable lazy queues for large queues
rabbitmqctl eval 'application:set_env(rabbit, lazy_queue_explicit_gc_run_operation_threshold, 1000).'
```

---

## âœ… **Best Practices**

### **1. Connection Management**
```python
# âœ… GOOD: Use connection pooling
class RabbitMQConnectionPool:
    def __init__(self, max_connections=10):
        self.pool = []
        self.max_connections = max_connections

    def get_connection(self):
        if len(self.pool) < self.max_connections:
            conn = pika.BlockingConnection(self._get_params())
            self.pool.append(conn)
            return conn
        return self.pool[0]

# âŒ BAD: Create new connection for each operation
def bad_practice():
    for i in range(100):
        connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
        # Do something
        connection.close()  # Creates connection overhead
```

### **2. Message Design**
```python
# âœ… GOOD: Structured message format
message = {
    'id': str(uuid.uuid4()),
    'action': 'process_data',
    'data': {'key': 'value'},
    'timestamp': datetime.now().isoformat(),
    'version': '1.0',
    'source': 'user_service'
}

# âŒ BAD: Unstructured message
message = "process_data|key=value|timestamp=2024-01-01"  # Hard to parse and extend
```

### **3. Error Handling**
```python
# âœ… GOOD: Comprehensive error handling
def process_message(ch, method, properties, body):
    try:
        data = json.loads(body)
        result = process_data(data)

        ch.basic_ack(delivery_tag=method.delivery_tag)
        return result
    except json.JSONDecodeError:
        # Invalid JSON - reject and don't requeue
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
    except ValueError as e:
        # Business logic error - reject and don't requeue
        log_error(f"Business logic error: {e}")
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
    except Exception as e:
        # Unexpected error - requeue for retry
        log_error(f"Unexpected error: {e}")
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)

# âŒ BAD: No error handling
def bad_error_handling(ch, method, properties, body):
    data = json.loads(body)  # Could fail
    process_data(data)       # Could fail
    ch.basic_ack(delivery_tag=method.delivery_tag)  # Never reached if error
```

### **4. Resource Management**
```python
# âœ… GOOD: Proper resource cleanup
class MessageProcessor:
    def __init__(self):
        self.connection = None
        self.channel = None

    def __enter__(self):
        self.connection = pika.BlockingConnection(self._get_params())
        self.channel = self.connection.channel()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.channel and not self.channel.is_closed:
            self.channel.close()
        if self.connection and not self.connection.is_closed:
            self.connection.close()

    def process(self, message):
        # Process message
        pass

# Usage
with MessageProcessor() as processor:
    processor.process(message)

# âŒ BAD: Resource leaks
def bad_resource_management():
    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
    channel = connection.channel()

    # Process messages
    for message in messages:
        channel.basic_publish(exchange='', routing_key='queue', body=message)

    # Forgot to close resources!
    # connection.close()
    # channel.close()
```

### **5. Monitoring and Alerting**
```python
# âœ… GOOD: Comprehensive monitoring
def monitor_queues():
    """Monitor queue health and send alerts"""
    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
    channel = connection.channel()

    # Check all queues
    queues = ['user_queue', 'email_queue', 'notification_queue']

    for queue_name in queues:
        try:
            queue_info = channel.queue_declare(queue=queue_name, passive=True)
            message_count = queue_info.method.message_count
            consumer_count = queue_info.method.consumer_count

            # Alert thresholds
            if message_count > 1000:
                alert(f"Queue {queue_name} has {message_count} messages!")
            if consumer_count == 0:
                alert(f"Queue {queue_name} has no consumers!")

        except Exception as e:
            alert(f"Failed to monitor queue {queue_name}: {e}")

    connection.close()

# Schedule monitoring
schedule.every(5).minutes.do(monitor_queues)
```

### **6. Performance Optimization**
```python
# âœ… GOOD: Optimized publishing
def optimized_publish(channel, messages, batch_size=100):
    """Batch publish messages for better performance"""
    for i in range(0, len(messages), batch_size):
        batch = messages[i:i + batch_size]

        # Publish batch
        for message in batch:
            channel.basic_publish(
                exchange=message['exchange'],
                routing_key=message['routing_key'],
                body=json.dumps(message['data']),
                properties=pika.BasicProperties(delivery_mode=2)
            )

# âœ… GOOD: Connection reuse
class RabbitMQClient:
    def __init__(self):
        self._connection = None
        self._channel = None

    @property
    def channel(self):
        if not self._connection or self._connection.is_closed:
            self._connection = pika.BlockingConnection(self._get_params())
            self._channel = self.connection.channel()
        return self._channel

    def close(self):
        if self._channel and not self._channel.is_closed:
            self._channel.close()
        if self._connection and not self._connection.is_closed:
            self._connection.close()

# Usage
client = RabbitMQClient()

# All operations use the same connection
client.channel.basic_publish(...)
client.channel.basic_consume(...)

# Close when done
client.close()
```

### **7. Security Best Practices**
```python
# âœ… GOOD: Secure connection
import ssl

ssl_options = {
    'certfile': '/path/to/client.crt',
    'keyfile': '/path/to/client.key',
    'ca_certs': '/path/to/ca.crt',
    'cert_reqs': ssl.CERT_REQUIRED
}

credentials = pika.PlainCredentials('username', 'password')
parameters = pika.ConnectionParameters(
    host='secure-rabbitmq.example.com',
    port=5671,  # TLS port
    credentials=credentials,
    ssl_options=ssl_options,
    heartbeat=600
)

# âœ… GOOD: Access control
# Use separate virtual hosts for different applications
connection = pika.BlockingConnection(
    pika.ConnectionParameters(
        host='localhost',
        virtual_host='/myapp',  # Isolated vhost
        credentials=pika.PlainCredentials('myapp_user', 'secure_password')
    )
)

# âœ… GOOD: Message encryption
import cryptography
from cryptography.fernet import Fernet

class SecureMessageHandler:
    def __init__(self, key):
        self.cipher = Fernet(key)

    def encrypt_message(self, message):
        return self.cipher.encrypt(json.dumps(message).encode())

    def decrypt_message(self, encrypted_message):
        return json.loads(self.cipher.decrypt(encrypted_message).decode())

# Usage
handler = SecureMessageHandler(b'your-encryption-key-here')
encrypted = handler.encrypt_message({'sensitive': 'data'})
# Publish encrypted message
```

### **8. Scalability Patterns**
```python
# âœ… GOOD: Horizontal scaling with consumers
def create_consumer_pool(queue_name, consumer_count=5):
    """Create multiple consumers for load distribution"""
    consumers = []

    for i in range(consumer_count):
        def create_consumer(consumer_id):
            def callback(ch, method, properties, body):
                print(f"Consumer {consumer_id} processing message")
                # Process message
                time.sleep(1)  # Simulate work
                ch.basic_ack(delivery_tag=method.delivery_tag)

            connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
            channel = connection.channel()

            # Set prefetch to distribute load evenly
            channel.basic_qos(prefetch_count=1)

            channel.basic_consume(queue=queue_name, on_message_callback=callback)
            consumers.append((connection, channel))

        create_consumer(i)

    return consumers

# âœ… GOOD: Publisher confirms for reliability
def publish_with_confirmation(channel, exchange, routing_key, message):
    """Publish with delivery confirmation"""
    # Enable publisher confirms
    channel.confirm_delivery()

    try:
        channel.basic_publish(
            exchange=exchange,
            routing_key=routing_key,
            body=json.dumps(message),
            properties=pika.BasicProperties(delivery_mode=2),
            mandatory=True  # Return message if no route
        )
        return True
    except pika.exceptions.UnroutableError:
        print("Message could not be routed")
        return False
    except Exception as e:
        print(f"Failed to publish: {e}")
        return False

# âœ… GOOD: Circuit breaker pattern
class CircuitBreaker:
    def __init__(self, failure_threshold=5, recovery_timeout=60):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN

    def call(self, func, *args, **kwargs):
        if self.state == 'OPEN':
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = 'HALF_OPEN'
            else:
                raise Exception("Circuit breaker is OPEN")

        try:
            result = func(*args, **kwargs)
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e

    def on_success(self):
        self.failure_count = 0
        self.state = 'CLOSED'

    def on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            self.state = 'OPEN'

# Usage
circuit_breaker = CircuitBreaker()

def publish_message(message):
    return circuit_breaker.call(
        lambda: channel.basic_publish(
            exchange='my_exchange',
            routing_key='my_key',
            body=json.dumps(message)
        )
    )
```

---

## ðŸŽ“ **Learning Path**

### **Beginner Level**
1. **Install RabbitMQ** using Docker
2. **Hello World** - Simple producer/consumer
3. **Work Queues** - Task distribution
4. **Publish/Subscribe** - Broadcasting messages
5. **Routing** - Direct and topic exchanges

### **Intermediate Level**
1. **RPC Pattern** - Request/response over messaging
2. **Message Acknowledgments** - Manual ACK/NACK
3. **Message Persistence** - Durable queues and messages
4. **Error Handling** - Dead letter exchanges
5. **Monitoring** - Management UI and basic metrics

### **Advanced Level**
1. **Clustering** - High availability setup
2. **Federation** - Cross-datacenter messaging
3. **Shovel** - Message replication
4. **Performance Tuning** - Connection pooling, batching
5. **Security** - TLS, authentication, authorization
6. **Custom Plugins** - Extend RabbitMQ functionality

### **Recommended Resources**
- ðŸ“š **Official Documentation**: https://www.rabbitmq.com/documentation.html
- ðŸŽ“ **RabbitMQ Tutorials**: https://www.rabbitmq.com/tutorials/
- ðŸ“– **RabbitMQ in Action** (Book)
- ðŸŽ¥ **YouTube Channels**:
  - RabbitMQ Official
  - CloudAMQP
  - DevOps Toolkit
- ðŸ›ï¸ **Community**: RabbitMQ Slack, Stack Overflow, GitHub

---

## ðŸŽ‰ **Conclusion**

RabbitMQ is a powerful, flexible, and reliable message broker that serves as the backbone for asynchronous communication in modern applications. This guide has covered everything from basic concepts to advanced production deployment patterns.

**Key Takeaways:**
- âœ… **Asynchronous Communication** enables scalable, decoupled systems
- âœ… **Multiple Exchange Types** provide flexible routing options
- âœ… **Message Persistence** ensures reliability and fault tolerance
- âœ… **Clustering** enables high availability and scalability
- âœ… **Rich Ecosystem** supports multiple programming languages
- âœ… **Production Ready** with comprehensive monitoring and management tools

**When to Use RabbitMQ:**
- Microservices communication
- Background job processing
- Event-driven architectures
- Real-time data streaming
- IoT message routing
- Legacy system integration

**Best Practices Summary:**
- Always use durable queues for production
- Implement proper error handling and dead letter exchanges
- Monitor queue lengths and consumer health
- Use connection pooling for better performance
- Implement message acknowledgments for reliability
- Set up proper logging and alerting
- Use virtual hosts for multi-tenancy
- Implement security best practices

This comprehensive guide should give you a solid foundation to start using RabbitMQ effectively in your projects. Remember, the key to mastering RabbitMQ is understanding the problem you're trying to solve and choosing the right messaging pattern for your use case.
